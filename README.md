# Async URL Collector

Небольшой **асинхронный** CLI‑проект для скачивания больших объёмов данных по списку URL:

- параллельные загрузки (лимит конкуренции)
- таймауты и ретраи
- аккуратные имена файлов
- сохранение **без пережатия** (байт‑в‑байт)
- `manifest.jsonl` с метаданными (URL → путь, размер, sha256, content-type, статус)

## Установка

```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

## Быстрый старт

1) Создай файл `urls.txt` (по одному URL на строку).

2) Запусти:

```bash
python -m async_url_collector download urls.txt --out downloads --concurrency 8 --timeout 30 --retries 3
```

Результат:
- файлы в `downloads/`
- лог‑манифест: `downloads/manifest.jsonl`

## Важно (качество изображений / авторские права)

- Этот инструмент **не сжимает** изображения. Если источник отдаёт оригинал — ты получишь оригинал.
- Telegram действительно может сжимать. Для “витрины” в TG лучше:
  - присылать **как файл (document)**, не “как фото”
  - хранить оригиналы у себя (диск/S3) и отдавать ссылки/превью
- Про “архив артов”: проверь права/лицензии/разрешение автора и условия площадок.


